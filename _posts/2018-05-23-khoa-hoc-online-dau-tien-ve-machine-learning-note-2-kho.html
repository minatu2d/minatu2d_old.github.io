---
layout: post
title: Khóa học online đầu tiên về Machine Learning (note 2) - Khó ~~
date: 2018-05-23 07:55:50.000000000 +09:00
type: post
parent_id: '0'
published: true
password: ''
status: publish
categories:
- Machine Learning
tags:
- CourserA
meta:
  _wpcom_is_markdown: '1'
  timeline_notification: '1527062153'
  _rest_api_published: '1'
  _rest_api_client_id: "-1"
  _publicize_job_id: '18159097208'
author:
  login: minatu
  email: pvta2pn@gmail.com
  display_name: minatu
  first_name: トウー
  last_name: フン・ヴァン
permalink: "/2018/05/23/khoa-hoc-online-dau-tien-ve-machine-learning-note-2-kho/"
---
<p>Tiếp theo <a href="https://lazytrick.wordpress.com/2018/05/23/khoa-hoc-online-dau-tien-ve-machine-learning-note-1-kho/" target="_blank">bài trước</a> trong loạt note về khóa học online đầu tiên về Machine Learning.</p>
<h2>Tuần 4 : Neurtal Networks : Biểu diễn</h2>
<p>Như đã biết, khi nói về dữ liệu, ta thường nói đến một loạt đặc trưng.<br />
Ví dụ: người thì có chiều cao, cân nặng, sở thích, etc.<br />
Và ta cũng đã biết về tạo cá đặc trưng đa thức khi các đặc trưng bậc 1 không đủ để miêu tả tốt dữ liệu.</p>
<p><!--more--></p>
<p>Giả sử, ta có một dữ liệu <strong>X</strong> có các đặc trưng <strong>x1, x2, x3, x4, x5, x6</strong>.<br />
Giờ ta cần thêm các đặc trưng dạng đa thức bậc 2, tức là ta phải thêm các dạng <strong>x1^2, x1x2, x2^2..</strong>.<br />
Sẽ có khoảng O(N^2) đặc trưng như thế. (Với N là số đặc trưng ban đầu).<br />
Khi N tăng, con số trên sẽ tăng rất nhanh.<br />
Khi số bậc tăng nữa, con số kia còn tăng nhanh hơn.<br />
=&gt; Gần như không thể sử dụng các biểu diễn này vi số chiều quá lớn. (???)</p>
<p>Và <strong>Neural Network</strong> là một cách biểu diễn thay thế khi có một <strong>hyphotheses</strong> với nhiều đặc trưng.<br />
Lấy ý tưởng từ việc mô phỏng đơn giản cách hoạt động của não bộ trong thần kinh học.</p>
<p>Tiép đến, nói về biểu diễn của <strong>Neural Network</strong>. Ta có thể hiểu là có nhiều đầu vào (mỗi đầu vào là một đặc trưng) được kết nối (các dây - hay chính là hệ số tuơng ứng với mỗi dây nối) đến 1 đầu ra thông qua một hàm (thường là không linear), để tạo ra chỉ một đầu ra duy nhất.<br />
Hàm được nói ở trên gọi là <strong>activation function</strong>. Thường là hàm <strong>sigmoid</strong> được nhắc đến ở bài trước.</p>
<p>Tuần này cũng nói đến cách biểu diễn thành các lớp.<br />
Lớp đầu tiên là Input (được đánh số là 1), mỗi giá tri ở lớp này tuơng ứng với một đặc trưng của đầu vào.<br />
Lớp cuối cùng là Output (gọi là lớp thứ L - giả sử có L lớp) chứa giá trị đầu sau khi qua các lớp ẩn.<br />
Các lớp ở giữa Input và Output là các lớp ẩn, được đánh số từ 2.<br />
Mỗi Neuron trong lớp ẩn và lớp Output là kết quả từ hàm <strong>activation</strong> của tổng  của tichtất cả các Neuron từ lớp ngay trước đó nhân kết nối tương ứng. Mỗi Neuron có tập kết nối khác nhau. Mỗi kết nối đó chính là một tham số cần phải được điều chỉnh trong quá trình <strong>Training</strong>.<br />
Ta biết hàm <strong>sigmoid</strong> hay bộ phân lớp sử dụng hàm <strong>sigmoid</strong> dùng để phân loại 2 lớp.<br />
Khi cần phân loại K lớp, ta cần K nàm <strong>sigmoid</strong> tuơng ứng thì ta cần <strong>K</strong> bộ phân lớp. Mỗi bộ phân lớp đó sẽ phân biệt một đầu vào thuộc lớp đó hay không, và xác suất là bao nhiêu.<br />
Tuơng ứng ta sẽ có <strong>K</strong> đầu ra ứng với <strong>K</strong> bộ phân lớp, K đầu ra này chính là đầu ra của lớp Output ở trên.</p>
<p>Trong tuần này, cũng nhắc đến cách biểu diễn <strong>one-hot</strong>, khi phân loai nhiều lớp.</p>
<h2>Tuần 5: Neural Network : Quá trình "học"</h2>
<p>Hàm <strong>Cost Function</strong>, hay tính khoảng cách giữa đầu ra dự đoán (giá trị tại lớp Output) với đầu ra thực tế. Có sử dụng <strong>Regularization</strong> để tránh Overfitting.<br />
Và nhớ là giá trị <strong>Cost Function</strong> là một số thực cho mỗi tập dữ liệu đầu vào.</p>
<p>Các tính <strong>Gradient</strong> cũng được làm tuơng tự. Bao gồm cảu <strong>Regularization</strong>. Số chiều của <strong>Gradient</strong> sẽ bằng số chiều tham số.</p>
<p>Hàm <strong>Cost Function</strong> tính được bằng khoảng các giữa đầu ra thật và đầu ra dự đoán chỉ dành cho lớp cuối cùng thôi.<br />
Nhưng ở Neural Network, có cả những lớp ẩn. Mỗi lớp ẩn có số Neuron khác nhau. Mỗi Neuron chính là một hàm với các kết nối có độ lớn khác nhau nữa.<br />
Ta phải đi update các hàm số (hay chính là độ lớp các kết nôi) của các hàm (hay các Neuron) đó nữa.<br />
Backgropagation Alogorithm sẽ thực hiện việc đó.<br />
Việc giải thích tại sao thuật toán trên lại chạy khá nặng về toán học, mình có đọc qua nhưng không nhớ được mấy. (Thấy nhiều i,j,k lắm, đau đầu thật).<br />
Đại khái là, nó sẽ tính độ lỗi hay <strong>mức độ phải trả giả cho cho việc dự đoán</strong> tại mỗi Neuron trong mỗi lớp ẩn. Việc tính này dựa vào độ lỗi từ lớp ngay sau.<br />
Từ giá trị đó sẽ tính tiếp các giá trị cho từng kết nối của Neuron đó (hệ số) tới các giá trị lớp ngay trước.<br />
Nghĩa là tính ngược từ lớp cuối lên. Tất nhiên là bỏ qua lớp Input.</p>
<p>Tiếp theo, việc tính đạo với hàm số phức tạp không hề dễ, lại còn dễ nhầm. Cần có một bước kiểm tra tính đúng đắn của giá trị đạo hàm. Cái này dựa trên tính chất của vốn có của đạo hàm để thực hiện một bước kiểm tra đạo hàm.</p>
<p>Tiếp theo, đến khởi tạo hệ số. Phải là random. Không thể khởi tạo toàn 0 cho các tham số được. Người ta thường khởi tạo giá trị trong một khoảng [-epsilon, epsilon] nào đó.</p>
